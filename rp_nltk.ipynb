{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rp-nltk.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6lrXJYOGvF+HBf/HIPfW2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marimuthuc/egen/blob/main/rp_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nhom1FsaBJF0"
      },
      "source": [
        "#Basics of NLP with NLTK Library\n",
        "This notebook explores the basics concepts like tokenizing, lemmatizing, stemming, etc with the help of NLTK library. \n",
        "> Reference: https://realpython.com/nltk-nlp-python/\n",
        "## Tokenizing\n",
        "> Splitting the text either by `word` or `sentence`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV3_f2xtAq6t",
        "outputId": "bbf7c660-57ed-4489-bb6b-008f4c9c2b35"
      },
      "source": [
        "# Import Libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFFGcFVTJD82",
        "outputId": "0fd191d5-ebe4-4d42-c749-d1c539af4d18"
      },
      "source": [
        "# word Tokenizing\n",
        "word_text = \"Words are like the atoms of natural language. They’re the smallest unit of meaning that still makes sense on its own. Tokenizing your text by word allows you to identify words that come up particularly often. For example, if you were analyzing a group of job ads, then you might find that the word “Python” comes up often. That could suggest high demand for Python knowledge, but you’d need to look deeper to know more.\"\n",
        "words = word_tokenize(word_text)\n",
        "print(words)\n",
        "len(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Words', 'are', 'like', 'the', 'atoms', 'of', 'natural', 'language', '.', 'They', '’', 're', 'the', 'smallest', 'unit', 'of', 'meaning', 'that', 'still', 'makes', 'sense', 'on', 'its', 'own', '.', 'Tokenizing', 'your', 'text', 'by', 'word', 'allows', 'you', 'to', 'identify', 'words', 'that', 'come', 'up', 'particularly', 'often', '.', 'For', 'example', ',', 'if', 'you', 'were', 'analyzing', 'a', 'group', 'of', 'job', 'ads', ',', 'then', 'you', 'might', 'find', 'that', 'the', 'word', '“', 'Python', '”', 'comes', 'up', 'often', '.', 'That', 'could', 'suggest', 'high', 'demand', 'for', 'Python', 'knowledge', ',', 'but', 'you', '’', 'd', 'need', 'to', 'look', 'deeper', 'to', 'know', 'more', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "89"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMPeEbt_DK1v",
        "outputId": "e8af7bcb-1769-441b-a39c-bd09965ce1ee"
      },
      "source": [
        "# Sentence Tokenizing\n",
        "sentence_text = \"When you tokenize by sentence, you can analyze how those words relate to one another and see more context. Are there a lot of negative words around the word “Python” because the hiring manager doesn’t like Python? Are there more terms from the domain of herpetology than the domain of software development, suggesting that you may be dealing with an entirely different kind of python than you were expecting?\"\n",
        "sentences = sent_tokenize(sentence_text)\n",
        "print(sentences)\n",
        "len(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['When you tokenize by sentence, you can analyze how those words relate to one another and see more context.', 'Are there a lot of negative words around the word “Python” because the hiring manager doesn’t like Python?', 'Are there more terms from the domain of herpetology than the domain of software development, suggesting that you may be dealing with an entirely different kind of python than you were expecting?']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smUSocbVJtiG"
      },
      "source": [
        "## Stop Words Removal\n",
        "> Removing common English words that are insignificant for analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ATSIqqyH9JH",
        "outputId": "06b791ed-3bd1-4e88-ce04-bf587769ea4e"
      },
      "source": [
        "# Downloading and importing stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"Stop words are words that you want to ignore, so you filter them out of your text when you’re processing it. Very common words like 'in', 'is', and 'an' are often used as stop words since they don’t add a lot of meaning to a text in and of themselves.\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "significant_words = [word for word in words if word.casefold() not in stop_words]\n",
        "print(significant_words)\n",
        "len(significant_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['Stop', 'words', 'words', 'want', 'ignore', ',', 'filter', 'text', '’', 'processing', '.', 'common', 'words', 'like', \"'in\", \"'\", ',', \"'is\", \"'\", ',', \"'an\", \"'\", 'often', 'used', 'stop', 'words', 'since', '’', 'add', 'lot', 'meaning', 'text', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnlhZsU6cGRV"
      },
      "source": [
        "## Stemming\n",
        "> Reducing words to their root (sometimes produce meaningless words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTbfk4GbLeO1",
        "outputId": "b2fed869-74e8-488e-c7a6-3bb07749b904"
      },
      "source": [
        "# Porter Stemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemming_text = \"Stemming is a text processing task in which you reduce words to their root, which is the core part of a word. For example, the words “helping” and “helper” share the root “help.” Stemming allows you to zero in on the basic meaning of a word rather than all the details of how it’s being used. NLTK has more than one stemmer, but you’ll be using the Porter stemmer.\"\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stemming_words = word_tokenize(stemming_text)\n",
        "\n",
        "stemmed_text = [stemmer.stem(word) for word in stemming_words]\n",
        "\n",
        "print(stemmed_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['stem', 'is', 'a', 'text', 'process', 'task', 'in', 'which', 'you', 'reduc', 'word', 'to', 'their', 'root', ',', 'which', 'is', 'the', 'core', 'part', 'of', 'a', 'word', '.', 'for', 'exampl', ',', 'the', 'word', '“', 'help', '”', 'and', '“', 'helper', '”', 'share', 'the', 'root', '“', 'help.', '”', 'stem', 'allow', 'you', 'to', 'zero', 'in', 'on', 'the', 'basic', 'mean', 'of', 'a', 'word', 'rather', 'than', 'all', 'the', 'detail', 'of', 'how', 'it', '’', 's', 'be', 'use', '.', 'nltk', 'ha', 'more', 'than', 'one', 'stemmer', ',', 'but', 'you', '’', 'll', 'be', 'use', 'the', 'porter', 'stemmer', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEf2BhSCfFXV"
      },
      "source": [
        "## Lemmatization\n",
        "> Reducing words to their `lemma` (base or dictionary form of a word)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLHHvDBddN0e"
      },
      "source": [
        "# WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}